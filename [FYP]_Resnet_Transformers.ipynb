{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEOYm5BoXMwd",
        "outputId": "fb47282e-2a80-4d57-9486-aa4d28c40bf5"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers -qqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tYgtUtVJRtgs"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tokenizers import Tokenizer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor\n",
        "import random\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j7dR-pLxRnof"
      },
      "outputs": [],
      "source": [
        "SEED = 2424\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXicnK6kUQ87",
        "outputId": "2c11e625-9533-450b-8fa3-e3c2976db758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5197\n"
          ]
        }
      ],
      "source": [
        "tokenizer_path =\"/home/ivlabs/Documents/Kshitij/archive/Flickr_tokenizer.json\"\n",
        "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
        "print(tokenizer.get_vocab_size())\n",
        "tokenizer.enable_padding(pad_id=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YNjCcIVVQRW6"
      },
      "outputs": [],
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "class CaptioningDataset(Dataset):\n",
        "  def __init__(self, split='train'):\n",
        "    super().__init__()\n",
        "    self.split=split\n",
        "    \n",
        "    data_path = \"/home/ivlabs/Documents/Kshitij/archive/captions.txt\"\n",
        "    self.images_path = \"/home/ivlabs/Documents/Kshitij/archive/Images/\"\n",
        "\n",
        "    with open(data_path) as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    lines = lines[1:]\n",
        "    random.shuffle(lines)\n",
        "    \n",
        "    images=[]\n",
        "    captions=[]\n",
        "\n",
        "    for some in lines:\n",
        "      i,c = some.split(',',1)\n",
        "      images.append(i)\n",
        "      captions.append(c.rstrip('\\n'))\n",
        "      \n",
        "    # images = images[1:]\n",
        "    # captions = captions[1:]\n",
        "    train_len = 30000\n",
        "\n",
        "    test_len = (len(captions) - train_len)//2\n",
        "    \n",
        "    if self.split=='train':\n",
        "      images = images[0:train_len]\n",
        "      captions = captions[0:train_len]\n",
        "\n",
        "    elif self.split=='test':\n",
        "      images = images[train_len:train_len+test_len]\n",
        "      captions = captions[train_len:train_len+test_len]\n",
        "\n",
        "    elif self.split=='validation':\n",
        "      images = images[train_len+test_len:train_len+(2*test_len)]\n",
        "      captions = captions[train_len+test_len:train_len+(2*test_len)]\n",
        "\n",
        "    self.images = images\n",
        "    self.captions = captions\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  # def __getitem__(self, index):\n",
        "  #   # print('here')\n",
        "  #   want_caption = self.captions[index]\n",
        "  #   want_image = self.images_path + self.images[index]\n",
        "  #   want_image = Image.open(want_image)\n",
        "  #   want_image = np.array(want_image.resize((224,224))).reshape(-1,224,224)\n",
        "  #   if want_image.shape[0]==1:\n",
        "  #     want_image = np.concatenate((want_image,want_image,want_image),axis=0)\n",
        "  #   # want_image = want_image.tolist()\n",
        "  #   # print('here now')\n",
        "  #   return want_image.tolist(), want_caption \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # print('here')\n",
        "    want_caption = self.captions[index]\n",
        "    want_image = self.images_path + self.images[index]\n",
        "    want_img_location = want_image\n",
        "    want_image = Image.open(want_image)\n",
        "    # want_image = np.array(want_image.resize((224,224))).reshape(224,224,-1)\n",
        "    # want_image = np.array(want_image.resize((224,224))).reshape(-1,224,224)\n",
        "    want_image = preprocess(want_image)\n",
        "    if want_image.shape[0]==1:\n",
        "      want_image = np.concatenate((want_image,want_image,want_image),axis=0)\n",
        "    # want_image = want_image.tolist()\n",
        "    # print('here now')\n",
        "    return want_image, want_caption, want_img_location\n",
        "\n",
        "class MyCollate:\n",
        "  def __init__(self):\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self,batch):\n",
        "    images=[]\n",
        "    captions=[]\n",
        "    image_locations= []\n",
        "    for i in batch:\n",
        "      # print(i)\n",
        "      images.append(i[0])\n",
        "      captions.append(i[1])\n",
        "      image_locations.append(i[2])\n",
        "    \n",
        "    # print(images)\n",
        "    # print(captions)\n",
        "    \n",
        "    captions = self.tokenizer.encode_batch(captions)\n",
        "\n",
        "    want_captions = []\n",
        "    attn = []\n",
        "\n",
        "    for i in captions:\n",
        "      # print(i.ids)\n",
        "      want_captions.append(i.ids)\n",
        "      attn.append(i.attention_mask)\n",
        "    # print(want_captions)\n",
        "    want_captions = torch.Tensor(want_captions).int()\n",
        "    attn = torch.Tensor(attn)\n",
        "    # images = torch.Tensor(images)\n",
        "    images = torch.stack(images)\n",
        "    return images, want_captions, attn.T, image_locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VXrfnSN3SwL1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 224, 224])\n",
            "torch.Size([64, 36])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset = CaptioningDataset(split='train')\n",
        "trainloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=MyCollate())\n",
        "DEVICE = 'cuda'\n",
        "for i in trainloader:\n",
        "  img = i[0]\n",
        "  # print(i[0].pixel_values.shape)\n",
        "  text = i[1]\n",
        "  # print(text.shape)\n",
        "  # print(text)\n",
        "  break\n",
        "print(img.shape)\n",
        "print(text.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "img_encoder_model = torchvision.models.resnet101(pretrained=True)\n",
        "img_encoder_model = torch.nn.Sequential(*(list(img_encoder_model.children())[:-2]))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, img_dim, num_proj_layers):\n",
        "    super().__init__()\n",
        "    self.resnet_dim = 2048\n",
        "    self.img_dim = img_dim\n",
        "    self.img_enc = img_encoder_model\n",
        "\n",
        "    self.num_proj_layers = num_proj_layers\n",
        "\n",
        "    layers = []\n",
        "    for i in range(num_proj_layers):\n",
        "      if i==0:\n",
        "        layers.append(nn.Sequential(nn.Linear(self.resnet_dim, self.img_dim),\n",
        "                                    nn.ReLU()\n",
        "                                    ))\n",
        "      else:\n",
        "        layers.append(nn.Sequential(nn.Linear(self.img_dim, self.img_dim),\n",
        "                                    nn.ReLU()\n",
        "                                    ))\n",
        "    self.layers = nn.ModuleList(layers)  \n",
        "  \n",
        "  def forward(self, img):\n",
        "    b = img.shape[0]\n",
        "    img = self.img_enc(img)\n",
        "    img = img.view(b,self.resnet_dim, -1).permute(0,2,1)\n",
        "    # print(img.shape)\n",
        "    img = img.squeeze()\n",
        "    for i in range(self.num_proj_layers):\n",
        "      img = self.layers[i](img)\n",
        "    return img.squeeze()\n",
        "\n",
        "# img_enc = Encoder(512,1)\n",
        "# i = {\"pixel_values\":torch.randn(64,3,224,224)}\n",
        "# out = img_enc(i)\n",
        "# print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KjTXHzRFXLqb"
      },
      "outputs": [],
      "source": [
        "class MultiHead_Attn_Layer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, dropout):\n",
        "        super(MultiHead_Attn_Layer, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hidden_dim // n_heads\n",
        "        self.fc_Q = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_K = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_V = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_O = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):                                    # [query] = [batch_size, query_len, hidden_dim] [key] = [batch_size, key_len, hidden_dim] [value] = [batch_Size, value_len, hidden_dim]\n",
        "        batch_size = query.shape[0]\n",
        "        # print(key.shape)\n",
        "        # print(query.shape)\n",
        "        # print(value.shape)                                           \n",
        "        Q = self.fc_Q(query)                                                            # [Q] = [batch_size, query_len, hidden_dim]   \n",
        "        K = self.fc_K(key)                                                              # [K] = [batch_size, key_len, hidden_dim]\n",
        "        V = self.fc_V(value)                                                            # [V] = [batch_size, value_len, hidden_dim]\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)     # [Q] = [batch_size, num_heads, query_len, head_dim]\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)     # [K] = [batch_size, num_heads, key_len, head_dim]\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)     # [V] = [batch_size, num_heads, value_len, head_dim]\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        if mask is not None:                                                            # [energy] = [batch_size, num_heads, query_len, key_len]  \n",
        "            energy = energy.masked_fill(mask == False, -1e10)                               \n",
        "        attention = torch.softmax(energy, dim = -1)                                     # [attention] = [batch_size, num_heads, query_len, key_len]\n",
        "        x = torch.matmul(self.dropout(attention), V)                                    # [x] = [batch_size, num_heads, query_len, head_dim]\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()                                          # [x] = [batch_size, query_len, num_heads, head_dim]\n",
        "        # Can avoid contiguous() if we use .reshape instead of .view in the next line\n",
        "        out = self.fc_O(x.view(batch_size, -1, self.hidden_dim))                        # [out] = [batch_size, query_len, hidden_dim]   \n",
        "        return out, attention\n",
        "\n",
        "class Postn_Feed_Fwrd(nn.Module):\n",
        "    def __init__(self, hidden_dim, pff_dim, dropout):\n",
        "        super(Postn_Feed_Fwrd, self).__init__()\n",
        "        self.fc1 = nn.Linear(hidden_dim, pff_dim)\n",
        "        self.fc2 = nn.Linear(pff_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input):                                                   # input = [batch_size, seq_len, hidden_dim]\n",
        "        out = torch.relu(self.fc1(input))                                       # out = [batch_size, seq_len, pff_dim]\n",
        "        out = self.fc2(self.dropout(out))                                       # out = [batch_size, seq_len, hidden_dim] \n",
        "        return out\n",
        "\n",
        "class Decoder_Layer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pff_dim, dropout):\n",
        "        super(Decoder_Layer, self).__init__()\n",
        "        self.self_attn = MultiHead_Attn_Layer(hidden_dim, n_heads, dropout)\n",
        "        self.cross_attn = MultiHead_Attn_Layer(hidden_dim, n_heads, dropout)\n",
        "        self.pff = Postn_Feed_Fwrd(hidden_dim, pff_dim, dropout)\n",
        "        self.attn_norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.attn_norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.pff_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, trg_mask, enc_out, src_mask):                            # trg = [batch_size, trg_len, hidden_dim] trg_mask = [batch_size, trg_len] enc_out = [batch_size, src_len, hidden_dim] src_mask = [batch_size, src_len]\n",
        "        sattn_out, _ = self.self_attn(trg, trg, trg, trg_mask)                      # satten_out = [batch_size, trg_len, hidden_dim]\n",
        "        inter_out1 = self.attn_norm1(self.dropout(sattn_out) + trg)                 # inter_out1 = [batch_size, trg_len, hidden_dim]  \n",
        "        cattn_out, attn = self.cross_attn(inter_out1, enc_out, enc_out, src_mask)   # cattn_out = [batch_size, trg_len, hidden_dim] attn = [batch_size, num_heads, query_len, key_len]\n",
        "        inter_out2 = self.attn_norm2(self.dropout(cattn_out) + inter_out1)          # inter_out2 = [batch_size, trg_len, hidden_dim]\n",
        "        pff_out = self.pff(inter_out2)                                              # pff_out = [batch_size, trg_len, hidden_dim]\n",
        "        out = self.pff_norm(self.dropout(pff_out) + inter_out2)                     # out = [batch_size, trg_len, hidden_dim]\n",
        "        return out, attn\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, tok_vocab_size, pos_vocab_size, hidden_dim, dec_heads, dec_pff_dim, num_layers, dec_dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.tok_embedding = nn.Embedding(tok_vocab_size, hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(pos_vocab_size, hidden_dim) #pos_vocab_size = Max possible sequence length\n",
        "        self.dec_layers = nn.ModuleList([Decoder_Layer(hidden_dim, dec_heads, dec_pff_dim, dec_dropout) for i in range(num_layers)])\n",
        "        self.fc = nn.Linear(hidden_dim, tok_vocab_size)\n",
        "        self.scale = math.sqrt(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dec_dropout)\n",
        "\n",
        "    def forward(self, trg, trg_mask, enc_out, src_mask):                                    # trg = [batch_size, trg_len] trg_mask = [batch_size, trg_len] enc_out = [] src_mask = []\n",
        "        batch_size = trg.shape[0] \n",
        "        trg_len = trg.shape[1]\n",
        "        tok_embed = self.tok_embedding(trg)                                                 # tok_embed = [batch_size, trg_len, hidden_dim]\n",
        "        pos_tensor = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device) # pos_tensor = [batch_size, trg_len]\n",
        "        pos_embed = self.pos_embedding(pos_tensor)                                          # pos_embed = [batch_size, trg_len, hidden_dim]\n",
        "        dec_embed = self.dropout(tok_embed * self.scale + pos_embed)                        # dec_embed = [batch_size, trg_len, hidden_dim]\n",
        "        dec_state = dec_embed\n",
        "        for dec_layer in self.dec_layers:\n",
        "            dec_state, attention = dec_layer(dec_state, trg_mask, enc_out, src_mask)        # dec_state = [batch_size, trg_len, hidden_dim] attention = [batch_size, num_heads, query_len, key_len]\n",
        "        out = self.fc(dec_state)                                                            # out = [batch_size, tok_vocab_size]\n",
        "        return out, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Qlbvt7xbUm38"
      },
      "outputs": [],
      "source": [
        "class Captioner(nn.Module):\n",
        "  def __init__(self, \n",
        "               img_dim,         #image encoder\n",
        "               num_proj_layers, #image encoder\n",
        "               tok_vocab_size,  #output vocab size\n",
        "               pos_vocab_size,  #max possible length of sentence\n",
        "               hidden_dim,      \n",
        "               dec_heads, \n",
        "               dec_pff_dim, \n",
        "               num_layers, \n",
        "               dec_dropout):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.trg_padding_idx = 4\n",
        "    self.image_encoder = Encoder(img_dim=img_dim, num_proj_layers=num_proj_layers)\n",
        "    self.language_model = Decoder(tok_vocab_size, pos_vocab_size, hidden_dim, dec_heads, dec_pff_dim, num_layers, dec_dropout)\n",
        "\n",
        "  def make_trg_mask(self, trg):                                                       # trg = [batch_size, trg_len]                  \n",
        "    trg_len = trg.shape[1] \n",
        "    pad_mask = (trg != self.trg_padding_idx).unsqueeze(1).unsqueeze(2).to(device)   # pad_mask = [batch_size, 1, 1, trg_len]\n",
        "    sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = device)).bool()   # sub_mask = [trg_len, trg_len]\n",
        "    trg_mask = pad_mask & sub_mask                                                  # trg_mask = [batch_size, 1, trg_len, trg_len]\n",
        "    return trg_mask\n",
        "\n",
        "  def make_src_mask(self, src):                                                       # src = [batch_size, src_len]\n",
        "        # src_mask = (src != self.src_padding_idx).unsqueeze(1).unsqueeze(2).to(device)   # src_mask = [batch_size, 1, 1, src_len]\n",
        "        src_mask = torch.ones(src.shape[0],1,1,src.shape[1]).bool().to(device)\n",
        "        return src_mask\n",
        "\n",
        "  def forward(self, image, caption, train=True):\n",
        "    img = self.image_encoder(image)\n",
        "    if train:\n",
        "      trg_mask = self.make_trg_mask(caption)\n",
        "    else:\n",
        "      # print(caption.shape)\n",
        "      trg_len = caption.shape[1]\n",
        "      pad_mask = (caption != self.trg_padding_idx).unsqueeze(1).unsqueeze(2).to(device)   # pad_mask = [batch_size, 1, 1, trg_len]\n",
        "      sub_mask = torch.ones(trg_len, trg_len).bool().to(device)   # sub_mask = [trg_len, trg_len]\n",
        "      trg_mask = pad_mask & sub_mask               \n",
        "    src_mask = self.make_src_mask(img)\n",
        "    output, attention = self.language_model(caption, trg_mask, img, src_mask)\n",
        "    return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nD27xh8z7Mdw"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, iterator, clip=1, device='cuda'):\n",
        "  epoch_loss=0\n",
        "  for data in iterator:\n",
        "    img = data[0].to(device)\n",
        "    text = data[1].to(device)\n",
        "    # print(img.shape)\n",
        "    # print(text.shape)\n",
        "    # print(text.shape)\n",
        "    optimizer.zero_grad()\n",
        "    model_input_text = text[:,:-1]\n",
        "    model_output_text = text[:,1:]\n",
        "    output, _ = model(img, model_input_text)\n",
        "    output = output.view(-1, output.shape[-1])\n",
        "    # print(output.shape)\n",
        "    model_output_text = model_output_text.contiguous().view(-1)\n",
        "    batch_loss = criterion(output, model_output_text.to(device).long())\n",
        "    batch_loss.backward()\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += batch_loss.item()\n",
        "\n",
        "  return epoch_loss/len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vUEHOql5hokw"
      },
      "outputs": [],
      "source": [
        "def Epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return (elapsed_mins, elapsed_secs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7SNLBNB5h1XC"
      },
      "outputs": [],
      "source": [
        "LR = 1e-3\n",
        "model = Captioner(img_dim=256,         #image encoder\n",
        "               num_proj_layers=1, #image encoder\n",
        "               tok_vocab_size=5197,  #output vocab size\n",
        "               pos_vocab_size=150,  #max possible length of sentence\n",
        "               hidden_dim=256,      \n",
        "               dec_heads=4, \n",
        "               dec_pff_dim=128, \n",
        "               num_layers=6, \n",
        "               dec_dropout=0.1).to('cuda')\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 4)\n",
        "optimizer = optim.Adam(model.parameters(), LR)\n",
        "dataset = CaptioningDataset(split='train')\n",
        "trainloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=MyCollate())\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8oGGSQbviLyt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for epochs 1 to 1: 8m 5s\n",
            "Training Loss: 4.5211 \n",
            "Training PPL: 91.9398 \n",
            "Time taken for epochs 2 to 2: 8m 9s\n",
            "Training Loss: 3.8590 \n",
            "Training PPL: 47.4194 \n",
            "Time taken for epochs 3 to 3: 8m 1s\n",
            "Training Loss: 3.6771 \n",
            "Training PPL: 39.5333 \n",
            "Time taken for epochs 4 to 4: 8m 1s\n",
            "Training Loss: 3.5634 \n",
            "Training PPL: 35.2841 \n",
            "Time taken for epochs 5 to 5: 7m 59s\n",
            "Training Loss: 3.4723 \n",
            "Training PPL: 32.2100 \n",
            "Time taken for epochs 6 to 6: 7m 58s\n",
            "Training Loss: 3.3980 \n",
            "Training PPL: 29.9056 \n",
            "Time taken for epochs 7 to 7: 8m 0s\n",
            "Training Loss: 3.3344 \n",
            "Training PPL: 28.0608 \n",
            "Time taken for epochs 8 to 8: 7m 59s\n",
            "Training Loss: 3.2765 \n",
            "Training PPL: 26.4842 \n",
            "Time taken for epochs 9 to 9: 8m 4s\n",
            "Training Loss: 3.2285 \n",
            "Training PPL: 25.2427 \n",
            "Time taken for epochs 10 to 10: 7m 57s\n",
            "Training Loss: 3.1806 \n",
            "Training PPL: 24.0609 \n",
            "Time taken for epochs 11 to 11: 7m 59s\n",
            "Training Loss: 3.1374 \n",
            "Training PPL: 23.0450 \n",
            "Time taken for epochs 12 to 12: 7m 58s\n",
            "Training Loss: 3.0981 \n",
            "Training PPL: 22.1556 \n",
            "Time taken for epochs 13 to 13: 8m 3s\n",
            "Training Loss: 3.0617 \n",
            "Training PPL: 21.3643 \n",
            "Time taken for epochs 14 to 14: 7m 58s\n",
            "Training Loss: 3.0247 \n",
            "Training PPL: 20.5884 \n",
            "Time taken for epochs 15 to 15: 7m 57s\n",
            "Training Loss: 2.9948 \n",
            "Training PPL: 19.9820 \n",
            "Time taken for epochs 16 to 16: 7m 58s\n",
            "Training Loss: 2.9650 \n",
            "Training PPL: 19.3944 \n",
            "Time taken for epochs 17 to 17: 8m 2s\n",
            "Training Loss: 2.9352 \n",
            "Training PPL: 18.8248 \n",
            "Time taken for epochs 18 to 18: 7m 57s\n",
            "Training Loss: 2.9107 \n",
            "Training PPL: 18.3695 \n",
            "Time taken for epochs 19 to 19: 7m 57s\n",
            "Training Loss: 2.8841 \n",
            "Training PPL: 17.8879 \n",
            "Time taken for epochs 20 to 20: 7m 57s\n",
            "Training Loss: 2.8615 \n",
            "Training PPL: 17.4881 \n",
            "Time taken for epochs 21 to 21: 7m 58s\n",
            "Training Loss: 2.8390 \n",
            "Training PPL: 17.0983 \n",
            "Time taken for epochs 22 to 22: 7m 56s\n",
            "Training Loss: 2.8170 \n",
            "Training PPL: 16.7263 \n",
            "Time taken for epochs 23 to 23: 7m 56s\n",
            "Training Loss: 2.7954 \n",
            "Training PPL: 16.3695 \n",
            "Time taken for epochs 24 to 24: 7m 56s\n",
            "Training Loss: 2.7723 \n",
            "Training PPL: 15.9960 \n",
            "Time taken for epochs 25 to 25: 8m 3s\n",
            "Training Loss: 2.7568 \n",
            "Training PPL: 15.7501 \n",
            "Time taken for epochs 26 to 26: 7m 58s\n",
            "Training Loss: 2.7392 \n",
            "Training PPL: 15.4753 \n",
            "Time taken for epochs 27 to 27: 8m 2s\n",
            "Training Loss: 2.7226 \n",
            "Training PPL: 15.2206 \n",
            "Time taken for epochs 28 to 28: 7m 53s\n",
            "Training Loss: 2.7058 \n",
            "Training PPL: 14.9660 \n",
            "Time taken for epochs 29 to 29: 7m 55s\n",
            "Training Loss: 2.7011 \n",
            "Training PPL: 14.8957 \n",
            "Time taken for epochs 30 to 30: 7m 55s\n",
            "Training Loss: 2.6732 \n",
            "Training PPL: 14.4864 \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "MODEL_TYPE = \"Transformer\"\n",
        "OUTPUT_PATH = f\"/home/ivlabs/Documents/Kshitij/thanmay/models/{MODEL_TYPE}\"\n",
        "MODEL_STORE_PATH = os.path.join(OUTPUT_PATH,f\"{MODEL_TYPE}_checkpoint_epoch.pth\")\n",
        "EPOCH_SAVE = 4 # Save the model every EPOCH_SAVE epochs\n",
        "outfile = open(os.path.join(OUTPUT_PATH, f\"{MODEL_TYPE}_train_losses.txt\"), \"a\")\n",
        "# outfile.write(\"Training Loss\\tTraining PPL\\n\")\n",
        "\n",
        "# model.load_state_dict(torch.load(MODEL_STORE_PATH.replace(\"epoch\",str(40))))\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "min_losses = 100\n",
        "prev_epoch = 1\n",
        "# min_losses = [float('inf'), float('inf')]\n",
        "NUM_EPOCHS = 40\n",
        "start_time = time.time()\n",
        "for epoch in range(1,41):\n",
        "    train_loss = train(model, criterion=criterion, optimizer=optimizer, iterator=trainloader)\n",
        "    train_losses.append(train_loss)\n",
        "    if epoch % EPOCH_SAVE == 0:\n",
        "        torch.save(model.state_dict(), MODEL_STORE_PATH.replace(\"epoch\",str(epoch)))\n",
        "    elapsed_time = Epoch_time(start_time, time.time())\n",
        "    print(f\"Time taken for epochs {prev_epoch} to {epoch}: {elapsed_time[0]}m {elapsed_time[1]}s\")\n",
        "    start_time = time.time()\n",
        "    prev_epoch = epoch + 1\n",
        "    print(f\"Training Loss: {train_loss:.4f} \")\n",
        "    print(f\"Training PPL: {math.exp(train_loss):.4f} \")\n",
        "    outfile.write(f\"{train_loss:.4f}\\t{                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             math.exp(train_loss):.4f}\\n\")\n",
        "outfile.close()\n",
        "# print(f\"Model with Train Loss {min_losses[1]:.4f}, Validation Loss: {min_losses[0]:.4f} was saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def testing(model, iterator, tokenizer):\n",
        "    predictions = []\n",
        "    locations = []\n",
        "    captions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in enumerate(iterator):\n",
        "            batch_locations = data[1][-1]\n",
        "            img = data[1][0]\n",
        "            text = data[1][1].to(device)\n",
        "            batch_size = text.shape[0]\n",
        "            img = img.to(device)\n",
        "            output, _ = model(img, text, train=True)\n",
        "            output = torch.softmax(output, dim=-1)\n",
        "            output = torch.argmax(output, dim=-1)\n",
        "            predictions.extend(tokenizer.decode_batch(output.tolist()))\n",
        "            captions.extend(tokenizer.decode_batch(text.tolist()))\n",
        "            locations.extend(batch_locations)\n",
        "    \n",
        "    return predictions, locations, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "test_dataset = CaptioningDataset(split='test')\n",
        "testloader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=MyCollate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-27 19:43:56.827715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-27 19:43:57.916411: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib::/home/ivlabs/.mujoco/mjpro150/bin\n",
            "2023-03-27 19:43:57.916485: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib::/home/ivlabs/.mujoco/mjpro150/bin\n",
            "2023-03-27 19:43:57.916492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[nltk_data] Downloading package wordnet to /home/ivlabs/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/ivlabs/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/ivlabs/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# !pip install evaluate -qqq\n",
        "# !pip install rouge_score -qqq\n",
        "\n",
        "import evaluate\n",
        "\n",
        "meteor = evaluate.load('meteor')\n",
        "rouge = evaluate.load('rouge')\n",
        "bleu = evaluate.load('bleu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "MODEL_TYPE = \"Transformer\"\n",
        "OUTPUT_PATH = f\"/home/ivlabs/Documents/Kshitij/thanmay/models/{MODEL_TYPE}\"\n",
        "MODEL_STORE_PATH = os.path.join(OUTPUT_PATH,f\"{MODEL_TYPE}_checkpoint_epoch.pth\")\n",
        "EPOCH_SAVE = 4 # Save the model every EPOCH_SAVE epochs\n",
        "outfile = open(os.path.join(OUTPUT_PATH, f\"{MODEL_TYPE}_scores.txt\"), \"w\")\n",
        "outfile.write(\"EPOCH\\tBLEU\\tMETEOR\\tROUGE1\\nROUGE2\\tROUGE_L\\tROUGE_Lsum\\n\")\n",
        "\n",
        "NUM_EPOCHS = 40\n",
        "for epoch in range(EPOCH_SAVE, NUM_EPOCHS + 1, EPOCH_SAVE):\n",
        "    model.load_state_dict(torch.load(MODEL_STORE_PATH.replace(\"epoch\",str(epoch))))\n",
        "    predictions, locations, captions = testing(model,testloader,tokenizer)\n",
        "    bleu_results = bleu.compute(predictions=predictions, references=captions)\n",
        "    meteor_results = meteor.compute(predictions=predictions, references=captions)\n",
        "    rouge_results = rouge.compute(predictions=predictions, references=captions)\n",
        "    outfile.write(f\"{epoch}\\t{bleu_results['bleu']}\\t{meteor_results['meteor']}\\t{rouge_results['rouge1']}\\t{rouge_results['rouge2']}\\t{rouge_results['rougeL']}\\t{rouge_results['rougeLsum']}\\n\")    \n",
        "outfile.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('NLPenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a5a3e2fa3f3b6f5247617e9643ffc383f4aa97dbacad653299d42f75829ebebe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
