{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install necessary libraries for the first run\n",
        "Uncomment the cell below for installing the libraries. If this file is ran on Google Colab, only `transformers` library needs to be installed, rest all libraries are pre-installed in Google Colab "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --upgrade torchvision\n",
        "# !pip install transformers -qqq\n",
        "# !pip install evaluate -qqq\n",
        "# !pip install rouge_score -qqq\n",
        "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 -qqq"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing all necessary libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v6jjO5GMwqOe"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "import math\n",
        "import evaluate\n",
        "from torchvision.models import resnet50\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tokenizers import Tokenizer\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor\n",
        "import random\n",
        "from torchvision import transforms"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting device in use for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the tokenizer which was made for the Flickr dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INi3bvlJxMFk",
        "outputId": "a25112f7-5819-48fe-8c84-c1b7ceb8f8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2706\n"
          ]
        }
      ],
      "source": [
        "tokenizer_path = \"/home/ivlabs/Documents/Kshitij/archive/Flickr_tokenizer.json\"\n",
        "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
        "print(tokenizer.get_vocab_size())\n",
        "tokenizer.enable_padding(pad_id=4)\n",
        "# vars(Tokenizer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting the seed for reproducing the results over multiple runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zNyRt4VWwuZx"
      },
      "outputs": [],
      "source": [
        "SEED = 2424\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset and Dataloader\n",
        "For creating the tokenizer, comment out the `MyCollate()` class and run the next cell. After the tokenizer is created with the alias `tokenizer.json`, uncomment the `MyCollate()` class and rerun the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JGDzJe-xxPNU"
      },
      "outputs": [],
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "class CaptioningDataset(Dataset):\n",
        "  def __init__(self, split='train'):\n",
        "    super().__init__()\n",
        "    self.split=split\n",
        "    \n",
        "    data_path = \"/home/ivlabs/Documents/Kshitij/archive/captions.txt\"\n",
        "    self.images_path = \"/home/ivlabs/Documents/Kshitij/archive/Images/\"\n",
        "\n",
        "    with open(data_path) as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    lines = lines[1:]\n",
        "    random.shuffle(lines)\n",
        "    \n",
        "    images=[]\n",
        "    captions=[]\n",
        "\n",
        "    for some in lines:\n",
        "      i,c = some.split(',',1)\n",
        "      images.append(i)\n",
        "      captions.append(c.rstrip('\\n'))\n",
        "      \n",
        "    # images = images[1:]\n",
        "    # captions = captions[1:]\n",
        "    train_len = 30000\n",
        "\n",
        "    test_len = (len(captions) - train_len)//2\n",
        "    \n",
        "    if self.split=='train':\n",
        "      images = images[0:train_len]\n",
        "      captions = captions[0:train_len]\n",
        "\n",
        "    elif self.split=='test':\n",
        "      images = images[train_len:train_len+test_len]\n",
        "      captions = captions[train_len:train_len+test_len]\n",
        "\n",
        "    elif self.split=='validation':\n",
        "      images = images[train_len+test_len:train_len+(2*test_len)]\n",
        "      captions = captions[train_len+test_len:train_len+(2*test_len)]\n",
        "\n",
        "    self.images = images\n",
        "    self.captions = captions\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # print('here')\n",
        "    want_caption = self.captions[index]\n",
        "    want_image = self.images_path + self.images[index]\n",
        "    want_img_location = want_image\n",
        "    want_image = Image.open(want_image)\n",
        "    # want_image = np.array(want_image.resize((224,224))).reshape(224,224,-1)\n",
        "    # want_image = np.array(want_image.resize((224,224))).reshape(-1,224,224)\n",
        "    want_image = preprocess(want_image)\n",
        "    if want_image.shape[0]==1:\n",
        "      want_image = np.concatenate((want_image,want_image,want_image),axis=0)\n",
        "    # want_image = want_image.tolist()\n",
        "    # print('here now')\n",
        "    return want_image, want_caption, want_img_location\n",
        "\n",
        "class MyCollate:\n",
        "  def __init__(self):\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self,batch):\n",
        "    images=[]\n",
        "    captions=[]\n",
        "    image_locations= []\n",
        "    for i in batch:\n",
        "      # print(i)\n",
        "      images.append(i[0])\n",
        "      captions.append(i[1])\n",
        "      image_locations.append(i[2])\n",
        "    \n",
        "    # print(images)\n",
        "    # print(captions)\n",
        "    \n",
        "    captions = self.tokenizer.encode_batch(captions)\n",
        "\n",
        "    want_captions = []\n",
        "    attn = []\n",
        "\n",
        "    for i in captions:\n",
        "      # print(i.ids)\n",
        "      want_captions.append(i.ids)\n",
        "      attn.append(i.attention_mask)\n",
        "    # print(want_captions)\n",
        "    want_captions = torch.Tensor(want_captions).int()\n",
        "    attn = torch.Tensor(attn)\n",
        "    # images = torch.Tensor(images)\n",
        "    images = torch.stack(images)\n",
        "    # print(want_captions.shape)\n",
        "    return images, want_captions.T, attn.T, image_locations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uncomment the cell below for building the tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "# from tokenizers import Tokenizer, processors\n",
        "# from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
        "# from tokenizers.trainers import BpeTrainer, WordLevelTrainer,WordPieceTrainer, UnigramTrainer\n",
        "# from tokenizers.pre_tokenizers import Whitespace, BertPreTokenizer\n",
        "# dataset = CaptioningDataset()\n",
        "\n",
        "# with open(\"/home/ivlabs/Documents/Kshitij/archive/dataset.txt\",'a') as f:\n",
        "#   for i in dataset.captions:\n",
        "#     f.write(i)\n",
        "\n",
        "# unk_token = \"<UNK>\"  # token for unknown words\n",
        "# spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\",\"<PAD>\"]  # special tokens\n",
        "\n",
        "\n",
        "# tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
        "# tokenizer.pre_tokenizer = BertPreTokenizer()\n",
        "# trainer = WordLevelTrainer(special_tokens = spl_tokens,min_frequency=5)\n",
        "\n",
        "# files = [\"/home/ivlabs/Documents/Kshitij/archive/dataset.txt\"]\n",
        "# tokenizer.train(files, trainer)\n",
        "# cls_token_id = tokenizer.token_to_id(\"<CLS>\")\n",
        "# sep_token_id = tokenizer.token_to_id(\"<SEP>\")\n",
        "\n",
        "# # tokenizer.post_processor = processors.TemplateProcessing(\n",
        "# #     single=f\"<CLS>:0 $A:0 <SEP>:0\",\n",
        "# #     pair=f\"<CLS>:0 $A:0 <SEP>:0 $B:1 <SEP>:1\",\n",
        "# #     special_tokens=[(\"<CLS>\", cls_token_id), (\"<SEP>\", sep_token_id)],\n",
        "# # )\n",
        "# tokenizer.save(\"/home/ivlabs/Documents/Kshitij/archive/Flickr_tokenizer.json\")\n",
        "# print(\"Tokenizer saved\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the dataloader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bd1ZEsvIxR1S"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = CaptioningDataset(split='train')\n",
        "trainloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=MyCollate())\n",
        "DEVICE = 'cuda'\n",
        "for i in trainloader:\n",
        "  img = i[0]\n",
        "  text = i[1]\n",
        "  break\n",
        "print(img.shape)\n",
        "print(text.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the Image encoder and Text decoder\n",
        "The image encoder in this version of the model was chosen to be CNN (ResNet101) of which the last layer was removed to achieve a single 2048 dimensional embedding for each image. Different image encoders like ResNet10, ResNet50 etc. can also be used but in that case, the class variable `self.resnet_dim` has to be changed from 2048 to other respective dimensionalities. In this notebook, the text decoder is a Long Short Term Memory (LSTM) RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K0T4dVmw47v",
        "outputId": "e716f61e-4e3e-47d0-b2bd-064b0377d081"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ivlabs/anaconda3/envs/NLPenv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/ivlabs/anaconda3/envs/NLPenv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# img_encoder_model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "# img_encoder_model = torch.nn.Sequential(*(list(img_encoder_model.children())[:-1]))\n",
        "# # for params in img_encoder_model.parameters():\n",
        "# #         params.requires_grad = False\n",
        "img_encoder_model = torchvision.models.resnet101(pretrained=True)\n",
        "img_encoder_model = torch.nn.Sequential(*(list(img_encoder_model.children())[:-1]))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, img_dim, num_proj_layers):\n",
        "    super().__init__()\n",
        "    self.resnet_dim = 2048\n",
        "    self.img_dim = img_dim\n",
        "    self.img_enc = img_encoder_model\n",
        "\n",
        "    self.num_proj_layers = num_proj_layers\n",
        "\n",
        "    layers = []\n",
        "    for i in range(num_proj_layers):\n",
        "      if i==0:\n",
        "        layers.append(nn.Sequential(nn.Linear(self.resnet_dim, self.img_dim),\n",
        "                                    nn.ReLU()\n",
        "                                    ))\n",
        "      else:\n",
        "        layers.append(nn.Sequential(nn.Linear(self.img_dim, self.img_dim),\n",
        "                                    nn.ReLU()\n",
        "                                    ))\n",
        "    self.layers = nn.ModuleList(layers)  \n",
        "  \n",
        "  def forward(self, img):\n",
        "    b = img.shape[0]\n",
        "    img = self.img_enc(img)\n",
        "    img = img.squeeze()\n",
        "    # img = img.view(b,self.resnet_dim, -1).permute(0,2,1)\n",
        "    # img = img.squeeze()\n",
        "    for i in range(self.num_proj_layers):\n",
        "      img = self.layers[i](img)\n",
        "    return img.squeeze()\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dec_dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, dropout=dec_dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dec_dropout)\n",
        "\n",
        "    def forward(self, input=None, states=None, enc_output=None):\n",
        "        if input is None:\n",
        "          # print(\"input is NOne\")\n",
        "          embedding = enc_output\n",
        "        else:\n",
        "          # print(\"input is not NOne\")\n",
        "          embedding = self.dropout(self.embedding(input))                         # input = [1, batch_size]  embedding = [1, batch_size, embedding_dim]       \n",
        "        \n",
        "        if states is not None:\n",
        "          # print(states[0].shape)\n",
        "          # print(states[1].shape)\n",
        "          output, states = self.rnn(embedding, states)                            # output = [seq_len+1, batch_size, num_directions*hidden_dim]   **Here number of directions is 1\n",
        "        else:\n",
        "          output, states = self.rnn(embedding) \n",
        "        output = self.fc(output).unsqueeze(0)                                   # output = [seq_len+1, batch_size, vocab_size]\n",
        "\n",
        "        return output, states"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combining the image encoder and text decoder into a single model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HnvcjRV1w_vz"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    \n",
        "    def forward(self, source, target, teacher_forcing_ratio):\n",
        "        enc_output = self.encoder(source)                                       # source = [batch_size, ]  target = [batch_size, seq_len]  enc_output = [batch_size, img_dim\n",
        "        # dec_states = enc_states\n",
        "        batch_size = target.shape[1]                                                 \n",
        "        seq_len = target.shape[0]\n",
        "        predictions = torch.zeros(seq_len, batch_size, tokenizer.get_vocab_size()).to(device)\n",
        "        input = enc_output.unsqueeze(0)                                       # input = [1, batch_size]\n",
        "        # print(input.shape)\n",
        "        for t in range(seq_len):\n",
        "          # print(input)\n",
        "          if t==0:\n",
        "            # print(\"setting input None\")\n",
        "            output, dec_states = self.decoder(input=None, states=None, enc_output=input)\n",
        "            # print(output.shape)\n",
        "          else:\n",
        "            # print(\"entered this\")\n",
        "            # print(input)\n",
        "            output, dec_states = self.decoder(input, states=dec_states, enc_output=None) \n",
        "          output = output.squeeze()\n",
        "          # print(output.shape)             \n",
        "          predictions[t] = output.view(batch_size, self.decoder.output_dim)\n",
        "          if random.random() < teacher_forcing_ratio:\n",
        "              input = target[t].unsqueeze(0)\n",
        "          else:\n",
        "              input = output.argmax(-1).unsqueeze(0)\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the training and other helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-4IG61h2xBkF"
      },
      "outputs": [],
      "source": [
        "def Train(iterator, model, criterion, optimizer, clip=1):\n",
        "  model.train()\n",
        "  epoch_loss=0\n",
        "  for _, batch in enumerate(iterator):\n",
        "    model.zero_grad()\n",
        "    img = batch[0].to(device)\n",
        "    text = batch[1].to(device)\n",
        "    # img = img.pixel_values.to(device)\n",
        "    # img = {'pixel_values':img}\n",
        "    # print(text.shape)\n",
        "    model_input_text = text[:-1,:]\n",
        "    model_output_text = text[1:,:]\n",
        "    outputs = model(img, model_input_text, teacher_forcing_ratio=0.8)\n",
        "    outputs = outputs.view(-1, outputs.shape[-1])\n",
        "    model_output_text = model_output_text.contiguous().view(-1)\n",
        "    batch_loss = criterion(outputs, model_output_text.to(device).long())\n",
        "    batch_loss.backward()\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += batch_loss.item()\n",
        "    \n",
        "  return epoch_loss/len(iterator)\n",
        "\n",
        "def Epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return (elapsed_mins, elapsed_secs)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the hyperparameters, loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o9ix97b90xKP"
      },
      "outputs": [],
      "source": [
        "CLIP = 1\n",
        "NUM_EPOCHS = 20\n",
        "HIDDEN_DIM = 768\n",
        "TRG_VOCAB_SIZE = tokenizer.get_vocab_size()\n",
        "EMBEDDING_DIM = 768\n",
        "NUM_LAYERS = 4\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 4)\n",
        "encoder = Encoder(img_dim=768, num_proj_layers=2).to(device)\n",
        "decoder = Decoder(TRG_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DEC_DROPOUT).to(device)\n",
        "seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
        "print(f'The model has {count_parameters(seq2seq):,} trainable parameters.')\n",
        "LR = 0.0001\n",
        "optimizer = optim.Adam(seq2seq.parameters(), LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# if os.path.isfile(\"/home/ivlabs/Documents/Kshitij/model.pth\"):\n",
        "#     seq2seq.load_state_dict(torch.load(\"/home/ivlabs/Documents/Kshitij/model_reset18.pth\"))\n",
        "#     print(\"sucessfully loaded checkpoint\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Looping through the dataloader for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFiU_SvO2dyz",
        "outputId": "36247370-1897-4677-90a3-c814108df107"
      },
      "outputs": [],
      "source": [
        "MODEL_TYPE = \"LSTM\"\n",
        "OUTPUT_PATH = f\"/home/ivlabs/Documents/Kshitij/thanmay/models/{MODEL_TYPE}\"\n",
        "MODEL_STORE_PATH = os.path.join(OUTPUT_PATH,f\"{MODEL_TYPE}_checkpoint_epoch.pth\")\n",
        "EPOCH_SAVE = 4 # Save the model every EPOCH_SAVE epochs\n",
        "outfile = open(os.path.join(OUTPUT_PATH, f\"{MODEL_TYPE}_train_losses.txt\"), \"w\")\n",
        "outfile.write(\"Training Loss\\tTraining PPL\\n\")\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "min_losses = 100\n",
        "prev_epoch = 1\n",
        "# min_losses = [float('inf'), float('inf')]\n",
        "NUM_EPOCHS = 40\n",
        "start_time = time.time()\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_loss = Train(iterator=trainloader, model=seq2seq, criterion=criterion, optimizer=optimizer, clip=1)\n",
        "    train_losses.append(train_loss)\n",
        "    if epoch % EPOCH_SAVE == 0:\n",
        "        torch.save(seq2seq.state_dict(), MODEL_STORE_PATH.replace(\"epoch\",str(epoch)))\n",
        "    elapsed_time = Epoch_time(start_time, time.time())\n",
        "    print(f\"Time taken for epochs {prev_epoch} to {epoch}: {elapsed_time[0]}m {elapsed_time[1]}s\")\n",
        "    start_time = time.time()\n",
        "    prev_epoch = epoch + 1\n",
        "    print(f\"Training Loss: {train_loss:.4f} \")\n",
        "    print(f\"Training PPL: {math.exp(train_loss):.4f} \")\n",
        "    outfile.write(f\"{train_loss:.4f}\\t{math.exp(train_loss):.4f}\\n\")\n",
        "\n",
        "outfile.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the model for testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_TYPE = \"LSTM\"\n",
        "OUTPUT_PATH = f\"/home/ivlabs/Documents/Kshitij/thanmay/models/{MODEL_TYPE}\"\n",
        "MODEL_STORE_PATH = os.path.join(OUTPUT_PATH,f\"{MODEL_TYPE}_checkpoint_40.pth\")\n",
        "seq2seq.load_state_dict(torch.load(MODEL_STORE_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "test_dataset = CaptioningDataset(split='test')\n",
        "testloader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=MyCollate())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the testing code for getting the caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "MeK_lOiO20yL"
      },
      "outputs": [],
      "source": [
        "def testing(model, iterator, tokenizer):\n",
        "    predictions = []\n",
        "    locations = []\n",
        "    captions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(iterator):\n",
        "            img = batch[0].to(device)\n",
        "            text = batch[1].to(device) # shape = (trg len, batch_size)\n",
        "            target = batch[1].to(device)\n",
        "            batch_size = text.shape[1]\n",
        "            model_input_text = text[:-1, :]\n",
        "            model_output_text = text[1:, :]\n",
        "            outputs = model(img, model_input_text, teacher_forcing_ratio=0.8)\n",
        "            batch_locations = batch[-1]\n",
        "            # print(locations)\n",
        "            # print(\"===================\")\n",
        "            outputs = torch.softmax(outputs, dim=-1) # shape = (trg len, batch_size, vocab_size)\n",
        "            outputs = torch.argmax(outputs, dim=-1) # shape = (batch_size, trg len)\n",
        "            predictions.extend(tokenizer.decode_batch(outputs.T.tolist()))\n",
        "            captions.extend(tokenizer.decode_batch(text.T.tolist()))\n",
        "            locations.extend(batch_locations)\n",
        "        return predictions, locations, captions\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the model on METEOR, ROUGE and BLEU scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "meteor = evaluate.load('meteor')\n",
        "rouge = evaluate.load('rouge')\n",
        "bleu = evaluate.load('bleu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "MODEL_TYPE = \"LSTM\"\n",
        "OUTPUT_PATH = f\"/home/ivlabs/Documents/Kshitij/thanmay/models/{MODEL_TYPE}\"\n",
        "MODEL_STORE_PATH = os.path.join(OUTPUT_PATH,f\"{MODEL_TYPE}_checkpoint_epoch.pth\")\n",
        "EPOCH_SAVE = 4 # Save the model every EPOCH_SAVE epochs\n",
        "outfile = open(os.path.join(OUTPUT_PATH, f\"{MODEL_TYPE}_scores.txt\"), \"w\")\n",
        "outfile.write(\"EPOCH\\tBLEU\\tMETEOR\\tROUGE1\\nROUGE2\\tROUGE_L\\tROUGE_Lsum\\n\")\n",
        "\n",
        "NUM_EPOCHS = 40\n",
        "for epoch in range(EPOCH_SAVE, NUM_EPOCHS + 1, EPOCH_SAVE):\n",
        "    seq2seq.load_state_dict(torch.load(MODEL_STORE_PATH.replace(\"epoch\",str(epoch))))\n",
        "    predictions, locations, captions = testing(seq2seq,testloader,tokenizer)\n",
        "    bleu_results = bleu.compute(predictions=predictions, references=captions)\n",
        "    meteor_results = meteor.compute(predictions=predictions, references=captions)\n",
        "    rouge_results = rouge.compute(predictions=predictions, references=captions)\n",
        "    outfile.write(f\"{epoch}\\t{bleu_results['bleu']}\\t{meteor_results['meteor']}\\t{rouge_results['rouge1']}\\t{rouge_results['rouge2']}\\t{rouge_results['rougeL']}\\t{rouge_results['rougeLsum']}\\n\")    \n",
        "outfile.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "only one element tensors can be converted to Python scalars",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/ivlabs/Documents/Kshitij/thanmay/[FYP]_Resnet_LSTM.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivlabs/Documents/Kshitij/thanmay/%5BFYP%5D_Resnet_LSTM.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# captions = tokenizer.decode_batch(captions)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ivlabs/Documents/Kshitij/thanmay/%5BFYP%5D_Resnet_LSTM.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mTensor(captions))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivlabs/Documents/Kshitij/thanmay/%5BFYP%5D_Resnet_LSTM.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bleu_results \u001b[39m=\u001b[39m bleu\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mcaptions)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ivlabs/Documents/Kshitij/thanmay/%5BFYP%5D_Resnet_LSTM.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m meteor_results \u001b[39m=\u001b[39m meteor\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mcaptions)\n",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ],
      "source": [
        "# captions = tokenizer.decode_batch(captions)\n",
        "# print(torch.Tensor(captions))\n",
        "bleu_results = bleu.compute(predictions=predictions, references=captions)\n",
        "meteor_results = meteor.compute(predictions=predictions, references=captions)\n",
        "rouge_results = rouge.compute(predictions=predictions, references=captions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('NLPenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a5a3e2fa3f3b6f5247617e9643ffc383f4aa97dbacad653299d42f75829ebebe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
