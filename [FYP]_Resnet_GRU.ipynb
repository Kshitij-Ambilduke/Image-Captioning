{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install necessary libraries for the first run\n",
        "Uncomment the cell below for installing the libraries. If this file is ran on Google Colab, only `transformers` library needs to be installed, rest all libraries are pre-installed in Google Colab "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --upgrade torchvision\n",
        "# !pip install transformers -qqq\n",
        "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 -qqq"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing all necessary libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v6jjO5GMwqOe"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision.models import resnet50\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tokenizers import Tokenizer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor\n",
        "import random\n",
        "from torchvision import transforms"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting device in use for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the tokenizer which was made for the Flickr dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INi3bvlJxMFk",
        "outputId": "a25112f7-5819-48fe-8c84-c1b7ceb8f8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2706\n"
          ]
        }
      ],
      "source": [
        "tokenizer_path = \"/home/ivlabs/Documents/Kshitij/archive/Flickr_tokenizer.json\"\n",
        "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
        "print(tokenizer.get_vocab_size())\n",
        "tokenizer.enable_padding(pad_id=4)\n",
        "# vars(Tokenizer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting the seed for reproducing the results on multiple runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zNyRt4VWwuZx"
      },
      "outputs": [],
      "source": [
        "SEED = 2424\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset and Dataloader\n",
        "For creating the tokenizer, comment out the `MyCollate()` class and run the next cell. After the tokenizer is created with the alias `tokenizer.json`, uncomment the `MyCollate()` class and rerun the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "JGDzJe-xxPNU"
      },
      "outputs": [],
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "class CaptioningDataset(Dataset):\n",
        "  def __init__(self, split='train'):\n",
        "    super().__init__()\n",
        "    self.split=split\n",
        "    \n",
        "    data_path = \"/home/ivlabs/Documents/Kshitij/archive/captions.txt\"\n",
        "    self.images_path = \"/home/ivlabs/Documents/Kshitij/archive/Images/\"\n",
        "\n",
        "    with open(data_path) as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    lines = lines[1:]\n",
        "    random.shuffle(lines)\n",
        "    \n",
        "    images=[]\n",
        "    captions=[]\n",
        "\n",
        "    for some in lines:\n",
        "      i,c = some.split(',',1)\n",
        "      images.append(i)\n",
        "      captions.append(c.rstrip('\\n'))\n",
        "      \n",
        "    # images = images[1:]\n",
        "    # captions = captions[1:]\n",
        "    train_len = 30000\n",
        "\n",
        "    test_len = (len(captions) - train_len)//2\n",
        "    \n",
        "    if self.split=='train':\n",
        "      images = images[0:train_len]\n",
        "      captions = captions[0:train_len]\n",
        "\n",
        "    elif self.split=='test':\n",
        "      images = images[train_len:train_len+test_len]\n",
        "      captions = captions[train_len:train_len+test_len]\n",
        "\n",
        "    elif self.split=='validation':\n",
        "      images = images[train_len+test_len:train_len+(2*test_len)]\n",
        "      captions = captions[train_len+test_len:train_len+(2*test_len)]\n",
        "\n",
        "    self.images = images\n",
        "    self.captions = captions\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # print('here')\n",
        "    want_caption = self.captions[index]\n",
        "    want_image = self.images_path + self.images[index]\n",
        "    want_img_location = want_image\n",
        "    want_image = Image.open(want_image)\n",
        "    # want_image = np.array(want_image.resize((224,224))).reshape(224,224,-1)\n",
        "    # want_image = np.array(want_image.resize((224,224))).reshape(-1,224,224)\n",
        "    want_image = preprocess(want_image)\n",
        "    if want_image.shape[0]==1:\n",
        "      want_image = np.concatenate((want_image,want_image,want_image),axis=0)\n",
        "    # want_image = want_image.tolist()\n",
        "    # print('here now')\n",
        "    return want_image, want_caption, want_img_location\n",
        "\n",
        "class MyCollate:\n",
        "  def __init__(self):\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self,batch):\n",
        "    images=[]\n",
        "    captions=[]\n",
        "    image_locations= []\n",
        "    for i in batch:\n",
        "      # print(i)\n",
        "      images.append(i[0])\n",
        "      captions.append(i[1])\n",
        "      image_locations.append(i[2])\n",
        "    \n",
        "    # print(images)\n",
        "    # print(captions)\n",
        "    \n",
        "    captions = self.tokenizer.encode_batch(captions)\n",
        "\n",
        "    want_captions = []\n",
        "    attn = []\n",
        "\n",
        "    for i in captions:\n",
        "      # print(i.ids)\n",
        "      want_captions.append(i.ids)\n",
        "      attn.append(i.attention_mask)\n",
        "    # print(want_captions)\n",
        "    want_captions = torch.Tensor(want_captions).int()\n",
        "    attn = torch.Tensor(attn)\n",
        "    # images = torch.Tensor(images)\n",
        "    images = torch.stack(images)\n",
        "    # print(want_captions.shape)\n",
        "    return images, want_captions.T, attn.T, image_locations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uncomment the cell below for building the tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "# from tokenizers import Tokenizer, processors\n",
        "# from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
        "# from tokenizers.trainers import BpeTrainer, WordLevelTrainer,WordPieceTrainer, UnigramTrainer\n",
        "# from tokenizers.pre_tokenizers import Whitespace, BertPreTokenizer\n",
        "# dataset = CaptioningDataset()\n",
        "\n",
        "# with open(\"/home/ivlabs/Documents/Kshitij/archive/dataset.txt\",'a') as f:\n",
        "#   for i in dataset.captions:\n",
        "#     f.write(i)\n",
        "\n",
        "# unk_token = \"<UNK>\"  # token for unknown words\n",
        "# spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\",\"<PAD>\"]  # special tokens\n",
        "\n",
        "\n",
        "# tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
        "# tokenizer.pre_tokenizer = BertPreTokenizer()\n",
        "# trainer = WordLevelTrainer(special_tokens = spl_tokens,min_frequency=5)\n",
        "\n",
        "# files = [\"/home/ivlabs/Documents/Kshitij/archive/dataset.txt\"]\n",
        "# tokenizer.train(files, trainer)\n",
        "# cls_token_id = tokenizer.token_to_id(\"<CLS>\")\n",
        "# sep_token_id = tokenizer.token_to_id(\"<SEP>\")\n",
        "\n",
        "# # tokenizer.post_processor = processors.TemplateProcessing(\n",
        "# #     single=f\"<CLS>:0 $A:0 <SEP>:0\",\n",
        "# #     pair=f\"<CLS>:0 $A:0 <SEP>:0 $B:1 <SEP>:1\",\n",
        "# #     special_tokens=[(\"<CLS>\", cls_token_id), (\"<SEP>\", sep_token_id)],\n",
        "# # )\n",
        "# tokenizer.save(\"/home/ivlabs/Documents/Kshitij/archive/Flickr_tokenizer.json\")\n",
        "# print(\"Tokenizer saved\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the dataloader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "bd1ZEsvIxR1S"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = CaptioningDataset(split='train')\n",
        "trainloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=MyCollate())\n",
        "DEVICE = 'cuda'\n",
        "for i in trainloader:\n",
        "  img = i[0]\n",
        "  # print(i[0].pixel_values.shape)\n",
        "  text = i[1]\n",
        "  # print(text.shape)\n",
        "  # print(text)\n",
        "  break\n",
        "print(img.shape)\n",
        "print(text.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the Image encoder and Text decoder\n",
        "The image encoder in this version of the model was chosen to be CNN (ResNet101) of which the last layer was removed to achieve a single 2048 dimensional embedding for each image. Different image encoders like ResNet10, ResNet50 etc. can also be used but in that case, the class variable `self.resnet_dim` has to be changed from 2048 to other respective dimensionalities. In this notebook, the text decoder is a Gated Reccurent Unit (GRU) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K0T4dVmw47v",
        "outputId": "e716f61e-4e3e-47d0-b2bd-064b0377d081"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ivlabs/anaconda3/envs/NLPenv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/ivlabs/anaconda3/envs/NLPenv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# img_encoder_model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "# img_encoder_model = torch.nn.Sequential(*(list(img_encoder_model.children())[:-1]))\n",
        "# # for params in img_encoder_model.parameters():\n",
        "# #         params.requires_grad = False\n",
        "\n",
        "img_encoder_model = torchvision.models.resnet101(pretrained=True)\n",
        "img_encoder_model = torch.nn.Sequential(*(list(img_encoder_model.children())[:-1]))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, img_dim, num_proj_layers):\n",
        "    super().__init__()\n",
        "    self.resnet_dim = 2048\n",
        "    self.img_dim = img_dim\n",
        "    self.img_enc = img_encoder_model\n",
        "\n",
        "    self.num_proj_layers = num_proj_layers\n",
        "\n",
        "    layers = []\n",
        "    for i in range(num_proj_layers):\n",
        "      if i==0:\n",
        "        layers.append(nn.Sequential(nn.Linear(self.resnet_dim, self.img_dim),\n",
        "                                    nn.ReLU()\n",
        "                                    ))\n",
        "      else:\n",
        "        layers.append(nn.Sequential(nn.Linear(self.img_dim, self.img_dim),\n",
        "                                    nn.ReLU()\n",
        "                                    ))\n",
        "    self.layers = nn.ModuleList(layers)  \n",
        "  \n",
        "  def forward(self, img):\n",
        "    b = img.shape[0]\n",
        "    img = self.img_enc(img)\n",
        "    img = img.squeeze()\n",
        "    # img = img.view(b,self.resnet_dim, -1).permute(0,2,1)\n",
        "    # img = img.squeeze()\n",
        "    for i in range(self.num_proj_layers):\n",
        "      img = self.layers[i](img)\n",
        "    return img.squeeze()\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dec_dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, dropout=dec_dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dec_dropout)\n",
        "\n",
        "    def forward(self, input=None, states=None, enc_output=None):\n",
        "        if input is None:\n",
        "          # print(\"input is NOne\")\n",
        "          embedding = enc_output\n",
        "        else:\n",
        "          # print(\"input is not NOne\")\n",
        "          embedding = self.dropout(self.embedding(input))                         # input = [1, batch_size]  embedding = [1, batch_size, embedding_dim]       \n",
        "        \n",
        "        if states is not None:\n",
        "          # print(states[0].shape)\n",
        "          # print(states[1].shape)\n",
        "          output, states = self.rnn(embedding, states)                            # output = [seq_len+1, batch_size, num_directions*hidden_dim]   **Here number of directions is 1\n",
        "        else:\n",
        "          output, states = self.rnn(embedding) \n",
        "        output = self.fc(output).unsqueeze(0)                                   # output = [seq_len+1, batch_size, vocab_size]\n",
        "\n",
        "        return output, states"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combining the image encoder and text decoder into a single model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HnvcjRV1w_vz"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    \n",
        "    def forward(self, source, target, teacher_forcing_ratio):\n",
        "        enc_output = self.encoder(source)                                       # source = [batch_size, ]  target = [batch_size, seq_len]  enc_output = [batch_size, img_dim\n",
        "        # dec_states = enc_states\n",
        "        batch_size = target.shape[1]                                                 \n",
        "        seq_len = target.shape[0]\n",
        "        predictions = torch.zeros(seq_len, batch_size, tokenizer.get_vocab_size()).to(device)\n",
        "        input = enc_output.unsqueeze(0)                                       # input = [1, batch_size]\n",
        "        # print(input.shape)\n",
        "        for t in range(seq_len):\n",
        "          # print(input)\n",
        "          if t==0:\n",
        "            # print(\"setting input None\")\n",
        "            output, dec_states = self.decoder(input=None, states=None, enc_output=input)\n",
        "            # print(output.shape)\n",
        "          else:\n",
        "            # print(\"entered this\")\n",
        "            # print(input)\n",
        "            output, dec_states = self.decoder(input, states=dec_states, enc_output=None) \n",
        "          output = output.squeeze()\n",
        "          # print(output.shape)             \n",
        "          predictions[t] = output.view(batch_size, self.decoder.output_dim)\n",
        "          if random.random() < teacher_forcing_ratio:\n",
        "              input = target[t].unsqueeze(0)\n",
        "          else:\n",
        "              input = output.argmax(-1).unsqueeze(0)\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the training and other helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-4IG61h2xBkF"
      },
      "outputs": [],
      "source": [
        "def Train(iterator, model, criterion, optimizer, clip=1):\n",
        "  model.train()\n",
        "  epoch_loss=0\n",
        "  for _, batch in enumerate(iterator):\n",
        "    model.zero_grad()\n",
        "    img = batch[0].to(device)\n",
        "    text = batch[1].to(device)\n",
        "    # img = img.pixel_values.to(device)\n",
        "    # img = {'pixel_values':img}\n",
        "    # print(text.shape)\n",
        "    model_input_text = text[:-1,:]\n",
        "    model_output_text = text[1:,:]\n",
        "    outputs = model(img, model_input_text, teacher_forcing_ratio=0.8)\n",
        "    outputs = outputs.view(-1, outputs.shape[-1])\n",
        "    model_output_text = model_output_text.contiguous().view(-1)\n",
        "    batch_loss = criterion(outputs, model_output_text.to(device).long())\n",
        "    batch_loss.backward()\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += batch_loss.item()\n",
        "    \n",
        "  return epoch_loss/len(iterator)\n",
        "\n",
        "def Epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return (elapsed_mins, elapsed_secs)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the hyperparameters, loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "o9ix97b90xKP"
      },
      "outputs": [],
      "source": [
        "CLIP = 1\n",
        "NUM_EPOCHS = 20\n",
        "HIDDEN_DIM = 768\n",
        "TRG_VOCAB_SIZE = tokenizer.get_vocab_size()\n",
        "EMBEDDING_DIM = 768\n",
        "NUM_LAYERS = 4\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 4)\n",
        "encoder = Encoder(img_dim=768, num_proj_layers=2).to(device)\n",
        "decoder = Decoder(TRG_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DEC_DROPOUT).to(device)\n",
        "seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
        "print(f'The model has {count_parameters(seq2seq):,} trainable parameters.')\n",
        "LR = 0.0001\n",
        "optimizer = optim.Adam(seq2seq.parameters(), LR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Looping through the dataloader for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFiU_SvO2dyz",
        "outputId": "36247370-1897-4677-90a3-c814108df107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for epochs 1 to 1: 8m 18s\n",
            "Training Loss: 4.9493 \n",
            "Training PPL: 141.0814 \n",
            "Time taken for epochs 2 to 2: 8m 5s\n",
            "Training Loss: 4.5434 \n",
            "Training PPL: 94.0084 \n",
            "Time taken for epochs 3 to 3: 8m 7s\n",
            "Training Loss: 4.3783 \n",
            "Training PPL: 79.7019 \n",
            "Time taken for epochs 4 to 4: 8m 6s\n",
            "Training Loss: 4.2868 \n",
            "Training PPL: 72.7315 \n",
            "Time taken for epochs 5 to 5: 8m 7s\n",
            "Training Loss: 4.2178 \n",
            "Training PPL: 67.8809 \n",
            "Time taken for epochs 6 to 6: 8m 7s\n",
            "Training Loss: 4.1462 \n",
            "Training PPL: 63.1940 \n",
            "Time taken for epochs 7 to 7: 8m 7s\n",
            "Training Loss: 4.0902 \n",
            "Training PPL: 59.7536 \n",
            "Time taken for epochs 8 to 8: 8m 8s\n",
            "Training Loss: 4.0503 \n",
            "Training PPL: 57.4174 \n",
            "Time taken for epochs 9 to 9: 8m 6s\n",
            "Training Loss: 4.0096 \n",
            "Training PPL: 55.1252 \n",
            "Time taken for epochs 10 to 10: 8m 6s\n",
            "Training Loss: 3.9767 \n",
            "Training PPL: 53.3415 \n",
            "Time taken for epochs 11 to 11: 8m 7s\n",
            "Training Loss: 3.9539 \n",
            "Training PPL: 52.1371 \n",
            "Time taken for epochs 12 to 12: 8m 6s\n",
            "Training Loss: 3.9202 \n",
            "Training PPL: 50.4091 \n",
            "Time taken for epochs 13 to 13: 8m 8s\n",
            "Training Loss: 3.8801 \n",
            "Training PPL: 48.4278 \n",
            "Time taken for epochs 14 to 14: 8m 8s\n",
            "Training Loss: 3.8497 \n",
            "Training PPL: 46.9810 \n",
            "Time taken for epochs 15 to 15: 8m 8s\n",
            "Training Loss: 3.8169 \n",
            "Training PPL: 45.4644 \n",
            "Time taken for epochs 16 to 16: 8m 8s\n",
            "Training Loss: 3.7882 \n",
            "Training PPL: 44.1782 \n",
            "Time taken for epochs 17 to 17: 8m 6s\n",
            "Training Loss: 3.7574 \n",
            "Training PPL: 42.8351 \n",
            "Time taken for epochs 18 to 18: 8m 7s\n",
            "Training Loss: 3.7345 \n",
            "Training PPL: 41.8680 \n",
            "Time taken for epochs 19 to 19: 8m 7s\n",
            "Training Loss: 3.7033 \n",
            "Training PPL: 40.5830 \n",
            "Time taken for epochs 20 to 20: 8m 8s\n",
            "Training Loss: 3.6802 \n",
            "Training PPL: 39.6559 \n",
            "Time taken for epochs 21 to 21: 8m 7s\n",
            "Training Loss: 3.6570 \n",
            "Training PPL: 38.7451 \n",
            "Time taken for epochs 22 to 22: 8m 6s\n",
            "Training Loss: 3.6283 \n",
            "Training PPL: 37.6501 \n",
            "Time taken for epochs 23 to 23: 8m 9s\n",
            "Training Loss: 3.6237 \n",
            "Training PPL: 37.4751 \n",
            "Time taken for epochs 24 to 24: 8m 7s\n",
            "Training Loss: 3.5909 \n",
            "Training PPL: 36.2675 \n",
            "Time taken for epochs 25 to 25: 8m 6s\n",
            "Training Loss: 3.5774 \n",
            "Training PPL: 35.7795 \n",
            "Time taken for epochs 26 to 26: 8m 7s\n",
            "Training Loss: 3.5567 \n",
            "Training PPL: 35.0456 \n",
            "Time taken for epochs 27 to 27: 8m 7s\n",
            "Training Loss: 3.5339 \n",
            "Training PPL: 34.2582 \n",
            "Time taken for epochs 28 to 28: 8m 6s\n",
            "Training Loss: 3.5170 \n",
            "Training PPL: 33.6836 \n",
            "Time taken for epochs 29 to 29: 8m 7s\n",
            "Training Loss: 3.5065 \n",
            "Training PPL: 33.3326 \n",
            "Time taken for epochs 30 to 30: 8m 7s\n",
            "Training Loss: 3.4823 \n",
            "Training PPL: 32.5330 \n",
            "Time taken for epochs 31 to 31: 8m 7s\n",
            "Training Loss: 3.4755 \n",
            "Training PPL: 32.3124 \n",
            "Time taken for epochs 32 to 32: 8m 6s\n",
            "Training Loss: 3.4527 \n",
            "Training PPL: 31.5854 \n",
            "Time taken for epochs 33 to 33: 8m 7s\n",
            "Training Loss: 3.4367 \n",
            "Training PPL: 31.0831 \n",
            "Time taken for epochs 34 to 34: 8m 8s\n",
            "Training Loss: 3.4229 \n",
            "Training PPL: 30.6568 \n",
            "Time taken for epochs 35 to 35: 8m 7s\n",
            "Training Loss: 3.4070 \n",
            "Training PPL: 30.1737 \n",
            "Time taken for epochs 36 to 36: 8m 7s\n",
            "Training Loss: 3.4017 \n",
            "Training PPL: 30.0164 \n",
            "Time taken for epochs 37 to 37: 8m 9s\n",
            "Training Loss: 3.3712 \n",
            "Training PPL: 29.1138 \n",
            "Time taken for epochs 38 to 38: 8m 7s\n",
            "Training Loss: 3.3630 \n",
            "Training PPL: 28.8761 \n",
            "Time taken for epochs 39 to 39: 8m 7s\n",
            "Training Loss: 3.3436 \n",
            "Training PPL: 28.3202 \n",
            "Time taken for epochs 40 to 40: 8m 5s\n",
            "Training Loss: 3.3385 \n",
            "Training PPL: 28.1775 \n"
          ]
        }
      ],
      "source": [
        "\n",
        "MODEL_TYPE = \"GRU\"\n",
        "OUTPUT_PATH = f\"/home/ivlabs/Documents/Kshitij/thanmay/models/{MODEL_TYPE}\"\n",
        "MODEL_STORE_PATH = os.path.join(OUTPUT_PATH,f\"{MODEL_TYPE}_checkpoint_epoch.pth\")\n",
        "EPOCH_SAVE = 4 # Save the model every EPOCH_SAVE epochs\n",
        "outfile = open(os.path.join(OUTPUT_PATH, f\"{MODEL_TYPE}_train_losses.txt\"), \"w\")\n",
        "outfile.write(\"Training Loss\\tTraining PPL\\n\")\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "min_losses = 100\n",
        "prev_epoch = 1\n",
        "# min_losses = [float('inf'), float('inf')]\n",
        "NUM_EPOCHS = 40\n",
        "start_time = time.time()\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_loss = Train(iterator=trainloader, model=seq2seq, criterion=criterion, optimizer=optimizer, clip=1)\n",
        "    train_losses.append(train_loss)\n",
        "    if epoch % EPOCH_SAVE == 0:\n",
        "        torch.save(seq2seq.state_dict(), MODEL_STORE_PATH.replace(\"epoch\",str(epoch)))\n",
        "    elapsed_time = Epoch_time(start_time, time.time())\n",
        "    print(f\"Time taken for epochs {prev_epoch} to {epoch}: {elapsed_time[0]}m {elapsed_time[1]}s\")\n",
        "    start_time = time.time()\n",
        "    prev_epoch = epoch + 1\n",
        "    print(f\"Training Loss: {train_loss:.4f} \")\n",
        "    print(f\"Training PPL: {math.exp(train_loss):.4f} \")\n",
        "    outfile.write(f\"{train_loss:.4f}\\t{math.exp(train_loss):.4f}\\n\")\n",
        "\n",
        "outfile.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the model for testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_TYPE = \"GRU\"\n",
        "OUTPUT_PATH = f\"/home/ivlabs/Documents/Kshitij/thanmay/models/{MODEL_TYPE}\"\n",
        "MODEL_STORE_PATH = os.path.join(OUTPUT_PATH,f\"{MODEL_TYPE}_checkpoint_40.pth\")\n",
        "seq2seq.load_state_dict(\n",
        "    torch.load(MODEL_STORE_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "test_dataset = CaptioningDataset(split='test')\n",
        "testloader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=MyCollate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "MeK_lOiO20yL"
      },
      "outputs": [],
      "source": [
        "def testing(model, iterator, tokenizer):\n",
        "    predictions = []\n",
        "    locations = []\n",
        "    captions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(iterator):\n",
        "            img = batch[0].to(device)\n",
        "            text = batch[1].to(device) # shape = (trg len, batch_size)\n",
        "            target = batch[1].to(device)\n",
        "            batch_size = text.shape[1]\n",
        "            model_input_text = text[:-1, :]\n",
        "            model_output_text = text[1:, :]\n",
        "            outputs = model(img, model_input_text, teacher_forcing_ratio=0.8)\n",
        "            batch_locations = batch[-1]\n",
        "            # print(locations)\n",
        "            # print(\"===================\")\n",
        "            outputs = torch.softmax(outputs, dim=-1) # shape = (trg len, batch_size, vocab_size)\n",
        "            outputs = torch.argmax(outputs, dim=-1) # shape = (batch_size, trg len)\n",
        "            predictions.extend(tokenizer.decode_batch(outputs.T.tolist()))\n",
        "            captions.extend(tokenizer.decode_batch(text.T.tolist()))\n",
        "            locations.extend(batch_locations)\n",
        "        return predictions, locations, captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-27 07:34:16.957731: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-27 07:34:18.074717: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib::/home/ivlabs/.mujoco/mjpro150/bin\n",
            "2023-03-27 07:34:18.074779: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib::/home/ivlabs/.mujoco/mjpro150/bin\n",
            "2023-03-27 07:34:18.074785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[nltk_data] Downloading package wordnet to /home/ivlabs/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/ivlabs/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/ivlabs/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# !pip install evaluate -qqq\n",
        "# !pip install rouge_score -qqq\n",
        "\n",
        "import evaluate\n",
        "\n",
        "meteor = evaluate.load('meteor')\n",
        "rouge = evaluate.load('rouge')\n",
        "bleu = evaluate.load('bleu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_TYPE = \"GRU\"\n",
        "OUTPUT_PATH = f\"/home/ivlabs/Documents/Kshitij/thanmay/models/{MODEL_TYPE}\"\n",
        "MODEL_STORE_PATH = os.path.join(OUTPUT_PATH,f\"{MODEL_TYPE}_checkpoint_epoch.pth\")\n",
        "EPOCH_SAVE = 4 # Save the model every EPOCH_SAVE epochs\n",
        "outfile = open(os.path.join(OUTPUT_PATH, f\"{MODEL_TYPE}_scores.txt\"), \"w\")\n",
        "outfile.write(\"EPOCH\\tBLEU\\tMETEOR\\tROUGE1\\nROUGE2\\tROUGE_L\\tROUGE_Lsum\\n\")\n",
        "\n",
        "NUM_EPOCHS = 40\n",
        "for epoch in range(EPOCH_SAVE, NUM_EPOCHS + 1, EPOCH_SAVE):\n",
        "    seq2seq.load_state_dict(torch.load(MODEL_STORE_PATH.replace(\"epoch\",str(epoch))))\n",
        "    predictions, locations, captions = testing(seq2seq,testloader,tokenizer)\n",
        "    bleu_results = bleu.compute(predictions=predictions, references=captions)\n",
        "    meteor_results = meteor.compute(predictions=predictions, references=captions)\n",
        "    rouge_results = rouge.compute(predictions=predictions, references=captions)\n",
        "    outfile.write(f\"{epoch}\\t{bleu_results['bleu']}\\t{meteor_results['meteor']}\\t{rouge_results['rouge1']}\\t{rouge_results['rouge2']}\\t{rouge_results['rougeL']}\\t{rouge_results['rougeLsum']}\\n\")    \n",
        "outfile.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# captions = tokenizer.decode_batch(captions)\n",
        "# print(torch.Tensor(captions))\n",
        "bleu_results = bleu.compute(predictions=predictions, references=captions)\n",
        "meteor_results = meteor.compute(predictions=predictions, references=captions)\n",
        "rouge_results = rouge.compute(predictions=predictions, references=captions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('NLPenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a5a3e2fa3f3b6f5247617e9643ffc383f4aa97dbacad653299d42f75829ebebe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
